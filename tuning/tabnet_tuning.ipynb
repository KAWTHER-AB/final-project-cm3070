{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tuning with Optuna (Tabnet)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Defining the Search Space and Launching the Optimization Study**\n",
    "\n",
    "In this section, we tune the TabNet model using **Optuna** to automatically search for the best hyperparameters that maximize validation F1-score.\n",
    "\n",
    "We used:\n",
    "- `optuna.create_study(direction=\"maximize\")` to find hyperparameter combinations that boost F1\n",
    "- A search space including TabNet dimensions (`n_d`, `n_a`), number of steps, `gamma`, `lambda_sparse`, learning rate, etc.\n",
    "- `torch.optim.lr_scheduler.StepLR` as our learning rate scheduler\n",
    "- A custom objective function that returns validation F1-score\n",
    "\n",
    "We saved:\n",
    "- The full Optuna study (`tabnet_study.pkl`)\n",
    "- The best hyperparameter configuration (`tabnet_best_params.csv`)\n",
    "\n",
    "This step follows best practices from **Chapter 7 ‚Äì Advanced Deep Learning** in *Fran√ßois Chollet‚Äôs Deep Learning with Python*, especially around hyperparameter optimization and model generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning/tabnet_tuning.ipynb\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set path to root of the project (adjust if needed)\n",
    "project_root = os.path.abspath(\"..\")  # one level up\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: lightblue; font-size: 16px; font-weight: bold;'>üóÇÔ∏è Loaded the original non-SMOTE, scaled dataset from: artifacts/tabnet/data_scaled_nosmote_for_tabnet.pkl</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Imports and Setup\n",
    "import optuna\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "from models.tabnet_model import build_tabnet_model\n",
    "from training.tabnet_trainer import train_tabnet_model\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from utils.style_utils import styled_print\n",
    "from torch.optim import Adam, RMSprop\n",
    "from torch.optim.lr_scheduler import StepLR  \n",
    "\n",
    "\n",
    "\n",
    "# 2. Load the original non-SMOTE, scaled dataset\n",
    "data_path = \"../artifacts/tabnet/data_scaled_nosmote_for_tabnet.pkl\"\n",
    "X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test = joblib.load(data_path)\n",
    "\n",
    "styled_print(\"üóÇÔ∏è Loaded the original non-SMOTE, scaled dataset from: artifacts/tabnet/data_scaled_nosmote_for_tabnet.pkl\")\n",
    "\n",
    "#  3. Define Optuna Objective Function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_categorical(\"n_d\", [8, 16, 32, 64]),\n",
    "        \"n_a\": trial.suggest_categorical(\"n_a\", [8, 16, 32, 64]),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 7),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0, step=0.5),\n",
    "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-5, 1e-2, log=True),\n",
    "        \"optimizer_params\": dict(lr=trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)),\n",
    "        \"scheduler_params\": {\"step_size\": 10, \"gamma\": 0.95},\n",
    "        \"scheduler_fn\": torch.optim.lr_scheduler.StepLR,\n",
    "        \"mask_type\": \"entmax\",  # better for sparse data\n",
    "        \"device_name\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    }\n",
    "\n",
    "    model = TabNetClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train=X_train_scaled.values,\n",
    "        y_train=y_train.values,\n",
    "        eval_set=[(X_val_scaled.values, y_val.values)],\n",
    "        eval_metric=[\"auc\", \"f1_score\"],\n",
    "        patience=10,\n",
    "        max_epochs=50,\n",
    "        batch_size=256,\n",
    "        virtual_batch_size=128\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val_scaled.values)\n",
    "    score = f1_score(y_val, preds)\n",
    "\n",
    "    return score  # or ROC AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 04:55:27,175] A new study created in memory with name: no-name-2afb2dc4-2a33-443a-97ee-0679dc2d69e6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9f45452306494f85e1161f6a48d0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.07094 | val_0_auc: 0.63635 | val_0_f1_score: 0.40426 |  0:00:23s\n",
      "epoch 1  | loss: 0.02515 | val_0_auc: 0.66184 | val_0_f1_score: 0.35714 |  0:00:45s\n",
      "epoch 2  | loss: 0.01868 | val_0_auc: 0.84247 | val_0_f1_score: 0.47619 |  0:01:08s\n",
      "epoch 3  | loss: 0.01647 | val_0_auc: 0.83031 | val_0_f1_score: 0.672   |  0:01:31s\n",
      "epoch 4  | loss: 0.01501 | val_0_auc: 0.84646 | val_0_f1_score: 0.70492 |  0:01:56s\n",
      "epoch 5  | loss: 0.01433 | val_0_auc: 0.77067 | val_0_f1_score: 0.624   |  0:02:18s\n",
      "epoch 6  | loss: 0.01348 | val_0_auc: 0.84022 | val_0_f1_score: 0.56075 |  0:02:43s\n",
      "epoch 7  | loss: 0.0124  | val_0_auc: 0.91392 | val_0_f1_score: 0.73381 |  0:03:06s\n",
      "epoch 8  | loss: 0.01147 | val_0_auc: 0.54064 | val_0_f1_score: 0.27184 |  0:03:27s\n",
      "epoch 9  | loss: 0.01191 | val_0_auc: 0.77175 | val_0_f1_score: 0.35849 |  0:03:49s\n",
      "epoch 10 | loss: 0.01019 | val_0_auc: 0.90572 | val_0_f1_score: 0.75385 |  0:04:11s\n",
      "epoch 11 | loss: 0.00983 | val_0_auc: 0.9129  | val_0_f1_score: 0.69182 |  0:04:33s\n",
      "epoch 12 | loss: 0.01018 | val_0_auc: 0.8664  | val_0_f1_score: 0.70769 |  0:04:56s\n",
      "epoch 13 | loss: 0.00946 | val_0_auc: 0.91398 | val_0_f1_score: 0.66667 |  0:05:16s\n",
      "epoch 14 | loss: 0.00934 | val_0_auc: 0.90041 | val_0_f1_score: 0.68333 |  0:05:36s\n",
      "epoch 15 | loss: 0.00856 | val_0_auc: 0.85303 | val_0_f1_score: 0.44898 |  0:05:57s\n",
      "epoch 16 | loss: 0.00902 | val_0_auc: 0.93005 | val_0_f1_score: 0.68056 |  0:06:17s\n",
      "epoch 17 | loss: 0.0086  | val_0_auc: 0.94136 | val_0_f1_score: 0.72593 |  0:06:37s\n",
      "epoch 18 | loss: 0.00821 | val_0_auc: 0.89185 | val_0_f1_score: 0.62121 |  0:06:57s\n",
      "epoch 19 | loss: 0.00786 | val_0_auc: 0.89507 | val_0_f1_score: 0.69173 |  0:07:17s\n",
      "epoch 20 | loss: 0.00793 | val_0_auc: 0.94739 | val_0_f1_score: 0.72593 |  0:07:37s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_f1_score = 0.75385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 05:03:32,144] Trial 0 finished with value: 0.7538461538461538 and parameters: {'n_d': 64, 'n_a': 32, 'n_steps': 6, 'gamma': 1.0, 'lambda_sparse': 0.0018731207587726801, 'lr': 0.0006340178596401655}. Best is trial 0 with value: 0.7538461538461538.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.02383 | val_0_auc: 0.93012 | val_0_f1_score: 0.47059 |  0:00:12s\n",
      "epoch 1  | loss: 0.01056 | val_0_auc: 0.93348 | val_0_f1_score: 0.5     |  0:00:24s\n",
      "epoch 2  | loss: 0.00771 | val_0_auc: 0.94552 | val_0_f1_score: 0.72131 |  0:00:37s\n",
      "epoch 3  | loss: 0.00641 | val_0_auc: 0.96844 | val_0_f1_score: 0.77778 |  0:00:51s\n",
      "epoch 4  | loss: 0.0058  | val_0_auc: 0.95929 | val_0_f1_score: 0.7438  |  0:01:03s\n",
      "epoch 5  | loss: 0.00516 | val_0_auc: 0.96412 | val_0_f1_score: 0.80315 |  0:01:16s\n",
      "epoch 6  | loss: 0.00481 | val_0_auc: 0.9649  | val_0_f1_score: 0.76423 |  0:01:28s\n",
      "epoch 7  | loss: 0.00431 | val_0_auc: 0.95612 | val_0_f1_score: 0.81818 |  0:01:40s\n",
      "epoch 8  | loss: 0.00464 | val_0_auc: 0.96733 | val_0_f1_score: 0.68966 |  0:01:53s\n",
      "epoch 9  | loss: 0.00425 | val_0_auc: 0.97065 | val_0_f1_score: 0.7377  |  0:02:05s\n",
      "epoch 10 | loss: 0.00399 | val_0_auc: 0.97655 | val_0_f1_score: 0.81818 |  0:02:17s\n",
      "epoch 11 | loss: 0.00377 | val_0_auc: 0.96888 | val_0_f1_score: 0.74797 |  0:02:29s\n",
      "epoch 12 | loss: 0.00363 | val_0_auc: 0.97159 | val_0_f1_score: 0.77778 |  0:02:42s\n",
      "epoch 13 | loss: 0.00361 | val_0_auc: 0.96244 | val_0_f1_score: 0.80916 |  0:02:54s\n",
      "epoch 14 | loss: 0.00361 | val_0_auc: 0.96575 | val_0_f1_score: 0.7874  |  0:03:07s\n",
      "epoch 15 | loss: 0.00369 | val_0_auc: 0.96692 | val_0_f1_score: 0.7874  |  0:03:20s\n",
      "epoch 16 | loss: 0.00346 | val_0_auc: 0.97364 | val_0_f1_score: 0.76423 |  0:03:32s\n",
      "epoch 17 | loss: 0.00326 | val_0_auc: 0.97971 | val_0_f1_score: 0.83077 |  0:03:44s\n",
      "epoch 18 | loss: 0.00362 | val_0_auc: 0.97591 | val_0_f1_score: 0.82171 |  0:03:57s\n",
      "epoch 19 | loss: 0.0035  | val_0_auc: 0.9759  | val_0_f1_score: 0.8     |  0:04:09s\n",
      "epoch 20 | loss: 0.00344 | val_0_auc: 0.97576 | val_0_f1_score: 0.83333 |  0:04:21s\n",
      "epoch 21 | loss: 0.00336 | val_0_auc: 0.96102 | val_0_f1_score: 0.82443 |  0:04:33s\n",
      "epoch 22 | loss: 0.00333 | val_0_auc: 0.97265 | val_0_f1_score: 0.80315 |  0:04:44s\n",
      "epoch 23 | loss: 0.00319 | val_0_auc: 0.97011 | val_0_f1_score: 0.8125  |  0:04:55s\n",
      "epoch 24 | loss: 0.00329 | val_0_auc: 0.9739  | val_0_f1_score: 0.80315 |  0:05:07s\n",
      "epoch 25 | loss: 0.00328 | val_0_auc: 0.97634 | val_0_f1_score: 0.81538 |  0:05:18s\n",
      "epoch 26 | loss: 0.00316 | val_0_auc: 0.97444 | val_0_f1_score: 0.7907  |  0:05:30s\n",
      "epoch 27 | loss: 0.00321 | val_0_auc: 0.96914 | val_0_f1_score: 0.8     |  0:05:45s\n",
      "epoch 28 | loss: 0.00318 | val_0_auc: 0.96149 | val_0_f1_score: 0.8209  |  0:06:02s\n",
      "epoch 29 | loss: 0.00323 | val_0_auc: 0.97363 | val_0_f1_score: 0.82171 |  0:06:15s\n",
      "epoch 30 | loss: 0.00305 | val_0_auc: 0.97653 | val_0_f1_score: 0.80882 |  0:06:30s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_f1_score = 0.83333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 05:10:26,671] Trial 1 finished with value: 0.8333333333333334 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 4, 'gamma': 2.0, 'lambda_sparse': 0.006276234052405445, 'lr': 0.0056400114420726115}. Best is trial 1 with value: 0.8333333333333334.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.0501  | val_0_auc: 0.80959 | val_0_f1_score: 0.57143 |  0:00:30s\n",
      "epoch 1  | loss: 0.01287 | val_0_auc: 0.869   | val_0_f1_score: 0.66667 |  0:00:55s\n",
      "epoch 2  | loss: 0.0101  | val_0_auc: 0.92312 | val_0_f1_score: 0.64384 |  0:01:21s\n",
      "epoch 3  | loss: 0.00887 | val_0_auc: 0.93863 | val_0_f1_score: 0.74419 |  0:01:47s\n",
      "epoch 4  | loss: 0.00782 | val_0_auc: 0.95913 | val_0_f1_score: 0.69504 |  0:02:11s\n",
      "epoch 5  | loss: 0.00706 | val_0_auc: 0.89452 | val_0_f1_score: 0.69421 |  0:02:35s\n",
      "epoch 6  | loss: 0.00661 | val_0_auc: 0.93842 | val_0_f1_score: 0.66242 |  0:03:00s\n",
      "epoch 7  | loss: 0.00613 | val_0_auc: 0.95088 | val_0_f1_score: 0.67626 |  0:03:26s\n",
      "epoch 8  | loss: 0.0058  | val_0_auc: 0.92104 | val_0_f1_score: 0.75    |  0:03:51s\n",
      "epoch 9  | loss: 0.00538 | val_0_auc: 0.93066 | val_0_f1_score: 0.76423 |  0:04:15s\n",
      "epoch 10 | loss: 0.00528 | val_0_auc: 0.95356 | val_0_f1_score: 0.75758 |  0:04:42s\n",
      "epoch 11 | loss: 0.00493 | val_0_auc: 0.94498 | val_0_f1_score: 0.78462 |  0:05:08s\n",
      "epoch 12 | loss: 0.00494 | val_0_auc: 0.92479 | val_0_f1_score: 0.69492 |  0:05:35s\n",
      "epoch 13 | loss: 0.00496 | val_0_auc: 0.94181 | val_0_f1_score: 0.76667 |  0:06:02s\n",
      "epoch 14 | loss: 0.00465 | val_0_auc: 0.9506  | val_0_f1_score: 0.76596 |  0:06:27s\n",
      "epoch 15 | loss: 0.00438 | val_0_auc: 0.95575 | val_0_f1_score: 0.74194 |  0:06:53s\n",
      "epoch 16 | loss: 0.00447 | val_0_auc: 0.9661  | val_0_f1_score: 0.73846 |  0:07:19s\n",
      "epoch 17 | loss: 0.0043  | val_0_auc: 0.94589 | val_0_f1_score: 0.48936 |  0:07:45s\n",
      "epoch 18 | loss: 0.00457 | val_0_auc: 0.94464 | val_0_f1_score: 0.68456 |  0:08:11s\n",
      "epoch 19 | loss: 0.00423 | val_0_auc: 0.92679 | val_0_f1_score: 0.68657 |  0:08:37s\n",
      "epoch 20 | loss: 0.00395 | val_0_auc: 0.93826 | val_0_f1_score: 0.7377  |  0:09:04s\n",
      "epoch 21 | loss: 0.00376 | val_0_auc: 0.87678 | val_0_f1_score: 0.25882 |  0:09:29s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_f1_score = 0.78462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 05:20:26,091] Trial 2 finished with value: 0.7846153846153846 and parameters: {'n_d': 64, 'n_a': 64, 'n_steps': 6, 'gamma': 2.0, 'lambda_sparse': 0.00665999788846571, 'lr': 0.0018485713748730651}. Best is trial 1 with value: 0.8333333333333334.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.32257 | val_0_auc: 0.28813 | val_0_f1_score: 0.02778 |  0:00:18s\n",
      "epoch 1  | loss: 0.02314 | val_0_auc: 0.43393 | val_0_f1_score: 0.14815 |  0:00:37s\n",
      "epoch 2  | loss: 0.01809 | val_0_auc: 0.49042 | val_0_f1_score: 0.12346 |  0:00:54s\n",
      "epoch 3  | loss: 0.01416 | val_0_auc: 0.58987 | val_0_f1_score: 0.18182 |  0:01:13s\n",
      "epoch 4  | loss: 0.01213 | val_0_auc: 0.70482 | val_0_f1_score: 0.31111 |  0:01:30s\n",
      "epoch 5  | loss: 0.01041 | val_0_auc: 0.77503 | val_0_f1_score: 0.29907 |  0:01:46s\n",
      "epoch 6  | loss: 0.00989 | val_0_auc: 0.8098  | val_0_f1_score: 0.41818 |  0:02:02s\n",
      "epoch 7  | loss: 0.00909 | val_0_auc: 0.80947 | val_0_f1_score: 0.45045 |  0:02:18s\n",
      "epoch 8  | loss: 0.00907 | val_0_auc: 0.8457  | val_0_f1_score: 0.55652 |  0:02:34s\n",
      "epoch 9  | loss: 0.00867 | val_0_auc: 0.83892 | val_0_f1_score: 0.61017 |  0:02:50s\n",
      "epoch 10 | loss: 0.00803 | val_0_auc: 0.86756 | val_0_f1_score: 0.59016 |  0:03:06s\n",
      "epoch 11 | loss: 0.00764 | val_0_auc: 0.84094 | val_0_f1_score: 0.63415 |  0:03:22s\n",
      "epoch 12 | loss: 0.00735 | val_0_auc: 0.87737 | val_0_f1_score: 0.57944 |  0:03:37s\n",
      "epoch 13 | loss: 0.00719 | val_0_auc: 0.86087 | val_0_f1_score: 0.50877 |  0:03:52s\n",
      "epoch 14 | loss: 0.00664 | val_0_auc: 0.90344 | val_0_f1_score: 0.57407 |  0:04:08s\n",
      "epoch 15 | loss: 0.0064  | val_0_auc: 0.89702 | val_0_f1_score: 0.63333 |  0:04:23s\n",
      "epoch 16 | loss: 0.00616 | val_0_auc: 0.89486 | val_0_f1_score: 0.65116 |  0:04:39s\n",
      "epoch 17 | loss: 0.00648 | val_0_auc: 0.88624 | val_0_f1_score: 0.64062 |  0:04:54s\n",
      "epoch 18 | loss: 0.00586 | val_0_auc: 0.92516 | val_0_f1_score: 0.65116 |  0:05:10s\n",
      "epoch 19 | loss: 0.00575 | val_0_auc: 0.89987 | val_0_f1_score: 0.70769 |  0:05:26s\n",
      "epoch 20 | loss: 0.00498 | val_0_auc: 0.91825 | val_0_f1_score: 0.72    |  0:05:42s\n",
      "epoch 21 | loss: 0.00547 | val_0_auc: 0.92287 | val_0_f1_score: 0.67742 |  0:05:58s\n",
      "epoch 22 | loss: 0.00571 | val_0_auc: 0.90705 | val_0_f1_score: 0.67742 |  0:06:15s\n",
      "epoch 23 | loss: 0.00516 | val_0_auc: 0.92675 | val_0_f1_score: 0.63866 |  0:06:31s\n",
      "epoch 24 | loss: 0.00503 | val_0_auc: 0.93441 | val_0_f1_score: 0.69841 |  0:06:47s\n",
      "epoch 25 | loss: 0.00532 | val_0_auc: 0.93276 | val_0_f1_score: 0.69697 |  0:07:03s\n",
      "epoch 26 | loss: 0.00482 | val_0_auc: 0.92512 | val_0_f1_score: 0.69291 |  0:07:19s\n",
      "epoch 27 | loss: 0.00448 | val_0_auc: 0.92181 | val_0_f1_score: 0.71875 |  0:07:35s\n",
      "epoch 28 | loss: 0.0047  | val_0_auc: 0.91959 | val_0_f1_score: 0.69291 |  0:07:52s\n",
      "epoch 29 | loss: 0.00459 | val_0_auc: 0.92289 | val_0_f1_score: 0.67742 |  0:08:08s\n",
      "epoch 30 | loss: 0.00449 | val_0_auc: 0.9287  | val_0_f1_score: 0.72    |  0:08:24s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_f1_score = 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 05:29:10,304] Trial 3 finished with value: 0.72 and parameters: {'n_d': 64, 'n_a': 32, 'n_steps': 4, 'gamma': 1.5, 'lambda_sparse': 0.00011342536650419821, 'lr': 0.00015500344931567613}. Best is trial 1 with value: 0.8333333333333334.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.17099 | val_0_auc: 0.27258 | val_0_f1_score: 0.0     |  0:00:09s\n",
      "epoch 1  | loss: 0.02605 | val_0_auc: 0.50443 | val_0_f1_score: 0.15    |  0:00:19s\n",
      "epoch 2  | loss: 0.01751 | val_0_auc: 0.71247 | val_0_f1_score: 0.4     |  0:00:29s\n",
      "epoch 3  | loss: 0.01372 | val_0_auc: 0.81846 | val_0_f1_score: 0.52427 |  0:00:38s\n",
      "epoch 4  | loss: 0.01207 | val_0_auc: 0.87013 | val_0_f1_score: 0.74576 |  0:00:48s\n",
      "epoch 5  | loss: 0.0106  | val_0_auc: 0.89178 | val_0_f1_score: 0.63063 |  0:00:57s\n",
      "epoch 6  | loss: 0.00996 | val_0_auc: 0.893   | val_0_f1_score: 0.75806 |  0:01:07s\n",
      "epoch 7  | loss: 0.00931 | val_0_auc: 0.89367 | val_0_f1_score: 0.78462 |  0:01:17s\n",
      "epoch 8  | loss: 0.00872 | val_0_auc: 0.87333 | val_0_f1_score: 0.77165 |  0:01:26s\n",
      "epoch 9  | loss: 0.00832 | val_0_auc: 0.882   | val_0_f1_score: 0.73684 |  0:01:36s\n",
      "epoch 10 | loss: 0.00836 | val_0_auc: 0.88253 | val_0_f1_score: 0.68852 |  0:01:45s\n",
      "epoch 11 | loss: 0.00786 | val_0_auc: 0.88947 | val_0_f1_score: 0.7619  |  0:01:55s\n",
      "epoch 12 | loss: 0.00764 | val_0_auc: 0.8717  | val_0_f1_score: 0.784   |  0:02:04s\n",
      "epoch 13 | loss: 0.00764 | val_0_auc: 0.88895 | val_0_f1_score: 0.7541  |  0:02:14s\n",
      "epoch 14 | loss: 0.00716 | val_0_auc: 0.89039 | val_0_f1_score: 0.79365 |  0:02:24s\n",
      "epoch 15 | loss: 0.00704 | val_0_auc: 0.91456 | val_0_f1_score: 0.81538 |  0:02:33s\n",
      "epoch 16 | loss: 0.00702 | val_0_auc: 0.91709 | val_0_f1_score: 0.76033 |  0:02:43s\n",
      "epoch 17 | loss: 0.00672 | val_0_auc: 0.92765 | val_0_f1_score: 0.8125  |  0:02:52s\n",
      "epoch 18 | loss: 0.00669 | val_0_auc: 0.91521 | val_0_f1_score: 0.8125  |  0:03:02s\n",
      "epoch 19 | loss: 0.00641 | val_0_auc: 0.91089 | val_0_f1_score: 0.81538 |  0:03:12s\n",
      "epoch 20 | loss: 0.00648 | val_0_auc: 0.89603 | val_0_f1_score: 0.80916 |  0:03:21s\n",
      "epoch 21 | loss: 0.00617 | val_0_auc: 0.91765 | val_0_f1_score: 0.81818 |  0:03:31s\n",
      "epoch 22 | loss: 0.00603 | val_0_auc: 0.91895 | val_0_f1_score: 0.80303 |  0:03:41s\n",
      "epoch 23 | loss: 0.00588 | val_0_auc: 0.90656 | val_0_f1_score: 0.79699 |  0:03:50s\n",
      "epoch 24 | loss: 0.00584 | val_0_auc: 0.91342 | val_0_f1_score: 0.80597 |  0:04:00s\n",
      "epoch 25 | loss: 0.00569 | val_0_auc: 0.91439 | val_0_f1_score: 0.81203 |  0:04:09s\n",
      "epoch 26 | loss: 0.00554 | val_0_auc: 0.92838 | val_0_f1_score: 0.80882 |  0:04:19s\n",
      "epoch 27 | loss: 0.00564 | val_0_auc: 0.93104 | val_0_f1_score: 0.82443 |  0:04:28s\n",
      "epoch 28 | loss: 0.00563 | val_0_auc: 0.92951 | val_0_f1_score: 0.78195 |  0:04:38s\n",
      "epoch 29 | loss: 0.00544 | val_0_auc: 0.91884 | val_0_f1_score: 0.7907  |  0:04:48s\n",
      "epoch 30 | loss: 0.00544 | val_0_auc: 0.9364  | val_0_f1_score: 0.83077 |  0:04:57s\n",
      "epoch 31 | loss: 0.00529 | val_0_auc: 0.92896 | val_0_f1_score: 0.84211 |  0:05:07s\n",
      "epoch 32 | loss: 0.00511 | val_0_auc: 0.94027 | val_0_f1_score: 0.82963 |  0:05:17s\n",
      "epoch 33 | loss: 0.00499 | val_0_auc: 0.93455 | val_0_f1_score: 0.83333 |  0:05:26s\n",
      "epoch 34 | loss: 0.00508 | val_0_auc: 0.941   | val_0_f1_score: 0.84211 |  0:05:36s\n",
      "epoch 35 | loss: 0.00479 | val_0_auc: 0.94045 | val_0_f1_score: 0.83333 |  0:05:46s\n",
      "epoch 36 | loss: 0.00466 | val_0_auc: 0.94032 | val_0_f1_score: 0.82443 |  0:05:56s\n",
      "epoch 37 | loss: 0.00461 | val_0_auc: 0.93827 | val_0_f1_score: 0.83333 |  0:06:05s\n",
      "epoch 38 | loss: 0.00456 | val_0_auc: 0.93921 | val_0_f1_score: 0.82443 |  0:06:15s\n",
      "epoch 39 | loss: 0.00465 | val_0_auc: 0.94128 | val_0_f1_score: 0.81538 |  0:06:24s\n",
      "epoch 40 | loss: 0.00444 | val_0_auc: 0.93379 | val_0_f1_score: 0.82443 |  0:06:33s\n",
      "epoch 41 | loss: 0.00429 | val_0_auc: 0.9426  | val_0_f1_score: 0.82443 |  0:06:43s\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_f1_score = 0.84211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 05:36:08,124] Trial 4 finished with value: 0.8421052631578947 and parameters: {'n_d': 8, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0, 'lambda_sparse': 0.005221911135507731, 'lr': 0.00032460570676150257}. Best is trial 4 with value: 0.8421052631578947.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.05316 | val_0_auc: 0.84499 | val_0_f1_score: 0.4902  |  0:00:16s\n",
      "epoch 1  | loss: 0.01948 | val_0_auc: 0.88228 | val_0_f1_score: 0.32609 |  0:00:33s\n",
      "epoch 2  | loss: 0.01549 | val_0_auc: 0.90677 | val_0_f1_score: 0.54545 |  0:00:50s\n",
      "epoch 3  | loss: 0.01258 | val_0_auc: 0.89621 | val_0_f1_score: 0.60504 |  0:01:07s\n",
      "epoch 4  | loss: 0.01088 | val_0_auc: 0.94916 | val_0_f1_score: 0.672   |  0:01:24s\n",
      "epoch 5  | loss: 0.01013 | val_0_auc: 0.90816 | val_0_f1_score: 0.672   |  0:01:41s\n",
      "epoch 6  | loss: 0.00941 | val_0_auc: 0.92837 | val_0_f1_score: 0.62903 |  0:01:57s\n",
      "epoch 7  | loss: 0.0078  | val_0_auc: 0.95715 | val_0_f1_score: 0.63492 |  0:02:15s\n",
      "epoch 8  | loss: 0.00707 | val_0_auc: 0.95227 | val_0_f1_score: 0.70588 |  0:02:31s\n",
      "epoch 9  | loss: 0.00617 | val_0_auc: 0.9438  | val_0_f1_score: 0.72308 |  0:02:48s\n",
      "epoch 10 | loss: 0.0056  | val_0_auc: 0.93171 | val_0_f1_score: 0.68966 |  0:03:05s\n",
      "epoch 11 | loss: 0.00518 | val_0_auc: 0.92943 | val_0_f1_score: 0.69492 |  0:03:21s\n",
      "epoch 12 | loss: 0.00512 | val_0_auc: 0.9623  | val_0_f1_score: 0.62016 |  0:03:38s\n",
      "epoch 13 | loss: 0.00522 | val_0_auc: 0.97165 | val_0_f1_score: 0.68254 |  0:03:55s\n",
      "epoch 14 | loss: 0.0046  | val_0_auc: 0.9711  | val_0_f1_score: 0.68333 |  0:04:12s\n",
      "epoch 15 | loss: 0.00413 | val_0_auc: 0.97691 | val_0_f1_score: 0.64286 |  0:04:29s\n",
      "epoch 16 | loss: 0.00412 | val_0_auc: 0.96739 | val_0_f1_score: 0.70229 |  0:04:47s\n",
      "epoch 17 | loss: 0.00437 | val_0_auc: 0.96338 | val_0_f1_score: 0.58333 |  0:05:03s\n",
      "epoch 18 | loss: 0.00437 | val_0_auc: 0.96781 | val_0_f1_score: 0.62069 |  0:05:18s\n",
      "epoch 19 | loss: 0.00452 | val_0_auc: 0.97592 | val_0_f1_score: 0.70677 |  0:05:33s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_f1_score = 0.72308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 05:42:10,030] Trial 5 finished with value: 0.7230769230769231 and parameters: {'n_d': 32, 'n_a': 8, 'n_steps': 6, 'gamma': 1.0, 'lambda_sparse': 0.004354358263594677, 'lr': 0.002952142859579131}. Best is trial 4 with value: 0.8421052631578947.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.11109 | val_0_auc: 0.36989 | val_0_f1_score: 0.32609 |  0:00:12s\n",
      "epoch 1  | loss: 0.02073 | val_0_auc: 0.59435 | val_0_f1_score: 0.32609 |  0:00:24s\n",
      "epoch 2  | loss: 0.01566 | val_0_auc: 0.6312  | val_0_f1_score: 0.35789 |  0:00:37s\n",
      "epoch 3  | loss: 0.01429 | val_0_auc: 0.66343 | val_0_f1_score: 0.31915 |  0:00:50s\n",
      "epoch 4  | loss: 0.0106  | val_0_auc: 0.7913  | val_0_f1_score: 0.50467 |  0:01:04s\n",
      "epoch 5  | loss: 0.00945 | val_0_auc: 0.77053 | val_0_f1_score: 0.53913 |  0:01:18s\n",
      "epoch 6  | loss: 0.00952 | val_0_auc: 0.88736 | val_0_f1_score: 0.61538 |  0:01:31s\n",
      "epoch 7  | loss: 0.00742 | val_0_auc: 0.85735 | val_0_f1_score: 0.61789 |  0:01:45s\n",
      "epoch 8  | loss: 0.0071  | val_0_auc: 0.8757  | val_0_f1_score: 0.61157 |  0:01:59s\n",
      "epoch 9  | loss: 0.00736 | val_0_auc: 0.93229 | val_0_f1_score: 0.61157 |  0:02:13s\n",
      "epoch 10 | loss: 0.00634 | val_0_auc: 0.91694 | val_0_f1_score: 0.68908 |  0:02:26s\n",
      "epoch 11 | loss: 0.00654 | val_0_auc: 0.90261 | val_0_f1_score: 0.55046 |  0:02:39s\n",
      "epoch 12 | loss: 0.00662 | val_0_auc: 0.88413 | val_0_f1_score: 0.62712 |  0:02:53s\n",
      "epoch 13 | loss: 0.00657 | val_0_auc: 0.91006 | val_0_f1_score: 0.65152 |  0:03:07s\n",
      "epoch 14 | loss: 0.00588 | val_0_auc: 0.85893 | val_0_f1_score: 0.66667 |  0:03:20s\n",
      "epoch 15 | loss: 0.00581 | val_0_auc: 0.94468 | val_0_f1_score: 0.68852 |  0:03:33s\n",
      "epoch 16 | loss: 0.0059  | val_0_auc: 0.94663 | val_0_f1_score: 0.69421 |  0:03:46s\n",
      "epoch 17 | loss: 0.00555 | val_0_auc: 0.96299 | val_0_f1_score: 0.65546 |  0:04:00s\n",
      "epoch 18 | loss: 0.00529 | val_0_auc: 0.96412 | val_0_f1_score: 0.70769 |  0:04:13s\n",
      "epoch 19 | loss: 0.00523 | val_0_auc: 0.96598 | val_0_f1_score: 0.73171 |  0:04:26s\n",
      "epoch 20 | loss: 0.00497 | val_0_auc: 0.96967 | val_0_f1_score: 0.68293 |  0:04:39s\n",
      "epoch 21 | loss: 0.00501 | val_0_auc: 0.97183 | val_0_f1_score: 0.55556 |  0:04:51s\n",
      "epoch 22 | loss: 0.00543 | val_0_auc: 0.97101 | val_0_f1_score: 0.704   |  0:05:04s\n",
      "epoch 23 | loss: 0.00504 | val_0_auc: 0.97044 | val_0_f1_score: 0.76336 |  0:05:17s\n",
      "epoch 24 | loss: 0.00516 | val_0_auc: 0.95268 | val_0_f1_score: 0.62295 |  0:05:29s\n",
      "epoch 25 | loss: 0.00483 | val_0_auc: 0.95767 | val_0_f1_score: 0.75591 |  0:05:42s\n",
      "epoch 26 | loss: 0.00495 | val_0_auc: 0.97049 | val_0_f1_score: 0.78125 |  0:05:55s\n",
      "epoch 27 | loss: 0.00467 | val_0_auc: 0.97002 | val_0_f1_score: 0.76033 |  0:06:08s\n",
      "epoch 28 | loss: 0.00479 | val_0_auc: 0.96955 | val_0_f1_score: 0.736   |  0:06:21s\n",
      "epoch 29 | loss: 0.00472 | val_0_auc: 0.96162 | val_0_f1_score: 0.79032 |  0:06:33s\n",
      "epoch 30 | loss: 0.00443 | val_0_auc: 0.95638 | val_0_f1_score: 0.77863 |  0:06:46s\n",
      "epoch 31 | loss: 0.0045  | val_0_auc: 0.97417 | val_0_f1_score: 0.72593 |  0:06:59s\n",
      "epoch 32 | loss: 0.00437 | val_0_auc: 0.97451 | val_0_f1_score: 0.74074 |  0:07:12s\n",
      "epoch 33 | loss: 0.00444 | val_0_auc: 0.95875 | val_0_f1_score: 0.71318 |  0:07:25s\n",
      "epoch 34 | loss: 0.00417 | val_0_auc: 0.94843 | val_0_f1_score: 0.71212 |  0:07:38s\n",
      "epoch 35 | loss: 0.00432 | val_0_auc: 0.9725  | val_0_f1_score: 0.70677 |  0:07:50s\n",
      "epoch 36 | loss: 0.00444 | val_0_auc: 0.95971 | val_0_f1_score: 0.67626 |  0:08:03s\n",
      "epoch 37 | loss: 0.00417 | val_0_auc: 0.96383 | val_0_f1_score: 0.67143 |  0:08:16s\n",
      "epoch 38 | loss: 0.00396 | val_0_auc: 0.9773  | val_0_f1_score: 0.72868 |  0:08:29s\n",
      "epoch 39 | loss: 0.00395 | val_0_auc: 0.97557 | val_0_f1_score: 0.74016 |  0:08:42s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_f1_score = 0.79032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 05:51:15,124] Trial 6 finished with value: 0.7903225806451613 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 5, 'gamma': 1.0, 'lambda_sparse': 0.00036685753910227595, 'lr': 0.0007537588768137618}. Best is trial 4 with value: 0.8421052631578947.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.0211  | val_0_auc: 0.90155 | val_0_f1_score: 0.70492 |  0:00:15s\n",
      "epoch 1  | loss: 0.00649 | val_0_auc: 0.95511 | val_0_f1_score: 0.74603 |  0:00:30s\n",
      "epoch 2  | loss: 0.00546 | val_0_auc: 0.96531 | val_0_f1_score: 0.76423 |  0:00:45s\n",
      "epoch 3  | loss: 0.00508 | val_0_auc: 0.95593 | val_0_f1_score: 0.8     |  0:01:00s\n",
      "epoch 4  | loss: 0.00492 | val_0_auc: 0.9806  | val_0_f1_score: 0.75188 |  0:01:15s\n",
      "epoch 5  | loss: 0.00456 | val_0_auc: 0.96623 | val_0_f1_score: 0.80916 |  0:01:30s\n",
      "epoch 6  | loss: 0.00427 | val_0_auc: 0.96247 | val_0_f1_score: 0.8062  |  0:01:45s\n",
      "epoch 7  | loss: 0.00442 | val_0_auc: 0.94871 | val_0_f1_score: 0.76923 |  0:01:59s\n",
      "epoch 8  | loss: 0.00452 | val_0_auc: 0.95921 | val_0_f1_score: 0.7619  |  0:02:15s\n",
      "epoch 9  | loss: 0.00402 | val_0_auc: 0.95821 | val_0_f1_score: 0.81538 |  0:02:29s\n",
      "epoch 10 | loss: 0.00382 | val_0_auc: 0.95954 | val_0_f1_score: 0.8062  |  0:02:45s\n",
      "epoch 11 | loss: 0.00403 | val_0_auc: 0.95313 | val_0_f1_score: 0.58621 |  0:03:00s\n",
      "epoch 12 | loss: 0.00417 | val_0_auc: 0.95643 | val_0_f1_score: 0.80315 |  0:03:15s\n",
      "epoch 13 | loss: 0.00397 | val_0_auc: 0.96524 | val_0_f1_score: 0.79699 |  0:03:29s\n",
      "epoch 14 | loss: 0.00368 | val_0_auc: 0.95224 | val_0_f1_score: 0.83077 |  0:03:44s\n",
      "epoch 15 | loss: 0.00371 | val_0_auc: 0.96552 | val_0_f1_score: 0.81818 |  0:03:59s\n",
      "epoch 16 | loss: 0.00352 | val_0_auc: 0.96994 | val_0_f1_score: 0.79699 |  0:04:14s\n",
      "epoch 17 | loss: 0.00348 | val_0_auc: 0.96788 | val_0_f1_score: 0.8     |  0:04:28s\n",
      "epoch 18 | loss: 0.00377 | val_0_auc: 0.97722 | val_0_f1_score: 0.77778 |  0:04:43s\n",
      "epoch 19 | loss: 0.00351 | val_0_auc: 0.97494 | val_0_f1_score: 0.80597 |  0:04:58s\n",
      "epoch 20 | loss: 0.00333 | val_0_auc: 0.96475 | val_0_f1_score: 0.81203 |  0:05:13s\n",
      "epoch 21 | loss: 0.00352 | val_0_auc: 0.96834 | val_0_f1_score: 0.81203 |  0:05:28s\n",
      "epoch 22 | loss: 0.00343 | val_0_auc: 0.97455 | val_0_f1_score: 0.80303 |  0:05:43s\n",
      "epoch 23 | loss: 0.00342 | val_0_auc: 0.98149 | val_0_f1_score: 0.81538 |  0:05:57s\n",
      "epoch 24 | loss: 0.00335 | val_0_auc: 0.98189 | val_0_f1_score: 0.76423 |  0:06:12s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_f1_score = 0.83077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 05:57:54,781] Trial 7 finished with value: 0.8307692307692308 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 6, 'gamma': 2.0, 'lambda_sparse': 0.0031452972956356328, 'lr': 0.008870714624775414}. Best is trial 4 with value: 0.8421052631578947.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.06299 | val_0_auc: 0.10785 | val_0_f1_score: 0.0     |  0:00:22s\n",
      "epoch 1  | loss: 0.0346  | val_0_auc: 0.4705  | val_0_f1_score: 0.2     |  0:00:45s\n",
      "epoch 2  | loss: 0.02249 | val_0_auc: 0.41403 | val_0_f1_score: 0.24444 |  0:01:07s\n",
      "epoch 3  | loss: 0.01807 | val_0_auc: 0.57935 | val_0_f1_score: 0.2766  |  0:01:29s\n",
      "epoch 4  | loss: 0.01559 | val_0_auc: 0.75199 | val_0_f1_score: 0.55932 |  0:01:52s\n",
      "epoch 5  | loss: 0.01238 | val_0_auc: 0.72583 | val_0_f1_score: 0.592   |  0:02:14s\n",
      "epoch 6  | loss: 0.01068 | val_0_auc: 0.74394 | val_0_f1_score: 0.51667 |  0:02:36s\n",
      "epoch 7  | loss: 0.00967 | val_0_auc: 0.75267 | val_0_f1_score: 0.50847 |  0:02:58s\n",
      "epoch 8  | loss: 0.01011 | val_0_auc: 0.75329 | val_0_f1_score: 0.48387 |  0:03:21s\n",
      "epoch 9  | loss: 0.00923 | val_0_auc: 0.79983 | val_0_f1_score: 0.49123 |  0:03:44s\n",
      "epoch 10 | loss: 0.00912 | val_0_auc: 0.79018 | val_0_f1_score: 0.51282 |  0:04:07s\n",
      "epoch 11 | loss: 0.0089  | val_0_auc: 0.77859 | val_0_f1_score: 0.49573 |  0:04:29s\n",
      "epoch 12 | loss: 0.00868 | val_0_auc: 0.78671 | val_0_f1_score: 0.47059 |  0:04:52s\n",
      "epoch 13 | loss: 0.00812 | val_0_auc: 0.78439 | val_0_f1_score: 0.4918  |  0:05:14s\n",
      "epoch 14 | loss: 0.00793 | val_0_auc: 0.81101 | val_0_f1_score: 0.56    |  0:05:37s\n",
      "epoch 15 | loss: 0.00705 | val_0_auc: 0.88048 | val_0_f1_score: 0.61871 |  0:06:00s\n",
      "epoch 16 | loss: 0.0064  | val_0_auc: 0.87787 | val_0_f1_score: 0.63636 |  0:06:22s\n",
      "epoch 17 | loss: 0.00691 | val_0_auc: 0.85608 | val_0_f1_score: 0.60938 |  0:06:45s\n",
      "epoch 18 | loss: 0.00681 | val_0_auc: 0.88977 | val_0_f1_score: 0.67153 |  0:07:07s\n",
      "epoch 19 | loss: 0.00615 | val_0_auc: 0.86664 | val_0_f1_score: 0.64463 |  0:07:30s\n",
      "epoch 20 | loss: 0.00596 | val_0_auc: 0.88731 | val_0_f1_score: 0.69841 |  0:07:52s\n",
      "epoch 21 | loss: 0.00576 | val_0_auc: 0.88382 | val_0_f1_score: 0.70312 |  0:08:15s\n",
      "epoch 22 | loss: 0.00535 | val_0_auc: 0.88479 | val_0_f1_score: 0.67669 |  0:08:37s\n",
      "epoch 23 | loss: 0.00513 | val_0_auc: 0.90369 | val_0_f1_score: 0.70968 |  0:08:58s\n",
      "epoch 24 | loss: 0.00519 | val_0_auc: 0.91822 | val_0_f1_score: 0.71533 |  0:09:20s\n",
      "epoch 25 | loss: 0.00507 | val_0_auc: 0.93858 | val_0_f1_score: 0.70677 |  0:09:42s\n",
      "epoch 26 | loss: 0.00494 | val_0_auc: 0.89692 | val_0_f1_score: 0.62903 |  0:10:04s\n",
      "epoch 27 | loss: 0.00534 | val_0_auc: 0.88557 | val_0_f1_score: 0.55556 |  0:10:26s\n",
      "epoch 28 | loss: 0.00477 | val_0_auc: 0.92569 | val_0_f1_score: 0.67669 |  0:10:47s\n",
      "epoch 29 | loss: 0.00504 | val_0_auc: 0.92779 | val_0_f1_score: 0.72308 |  0:11:09s\n",
      "epoch 30 | loss: 0.00484 | val_0_auc: 0.80139 | val_0_f1_score: 0.50435 |  0:11:31s\n",
      "epoch 31 | loss: 0.00468 | val_0_auc: 0.86867 | val_0_f1_score: 0.65625 |  0:11:53s\n",
      "epoch 32 | loss: 0.00471 | val_0_auc: 0.84087 | val_0_f1_score: 0.59375 |  0:12:15s\n",
      "epoch 33 | loss: 0.00445 | val_0_auc: 0.87113 | val_0_f1_score: 0.62687 |  0:12:37s\n",
      "epoch 34 | loss: 0.00475 | val_0_auc: 0.86032 | val_0_f1_score: 0.58333 |  0:12:59s\n",
      "epoch 35 | loss: 0.00507 | val_0_auc: 0.91264 | val_0_f1_score: 0.68571 |  0:13:21s\n",
      "epoch 36 | loss: 0.00477 | val_0_auc: 0.9082  | val_0_f1_score: 0.6383  |  0:13:43s\n",
      "epoch 37 | loss: 0.00484 | val_0_auc: 0.91672 | val_0_f1_score: 0.71533 |  0:14:05s\n",
      "epoch 38 | loss: 0.0047  | val_0_auc: 0.89731 | val_0_f1_score: 0.69065 |  0:14:28s\n",
      "epoch 39 | loss: 0.00427 | val_0_auc: 0.90181 | val_0_f1_score: 0.68702 |  0:14:49s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_f1_score = 0.72308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 06:13:10,718] Trial 8 finished with value: 0.7230769230769231 and parameters: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.5, 'lambda_sparse': 1.64947322619163e-05, 'lr': 0.00013093000549781525}. Best is trial 4 with value: 0.8421052631578947.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.03955 | val_0_auc: 0.65386 | val_0_f1_score: 0.20455 |  0:00:13s\n",
      "epoch 1  | loss: 0.01255 | val_0_auc: 0.59192 | val_0_f1_score: 0.26415 |  0:00:25s\n",
      "epoch 2  | loss: 0.0095  | val_0_auc: 0.8388  | val_0_f1_score: 0.47273 |  0:00:38s\n",
      "epoch 3  | loss: 0.00779 | val_0_auc: 0.89188 | val_0_f1_score: 0.47863 |  0:00:51s\n",
      "epoch 4  | loss: 0.00651 | val_0_auc: 0.91396 | val_0_f1_score: 0.66102 |  0:01:04s\n",
      "epoch 5  | loss: 0.00537 | val_0_auc: 0.87722 | val_0_f1_score: 0.59504 |  0:01:17s\n",
      "epoch 6  | loss: 0.00594 | val_0_auc: 0.95052 | val_0_f1_score: 0.71429 |  0:01:30s\n",
      "epoch 7  | loss: 0.00503 | val_0_auc: 0.9571  | val_0_f1_score: 0.69919 |  0:01:43s\n",
      "epoch 8  | loss: 0.00437 | val_0_auc: 0.93685 | val_0_f1_score: 0.72581 |  0:01:57s\n",
      "epoch 9  | loss: 0.00418 | val_0_auc: 0.94165 | val_0_f1_score: 0.7377  |  0:02:10s\n",
      "epoch 10 | loss: 0.00411 | val_0_auc: 0.94447 | val_0_f1_score: 0.71429 |  0:02:23s\n",
      "epoch 11 | loss: 0.0039  | val_0_auc: 0.96144 | val_0_f1_score: 0.752   |  0:02:35s\n",
      "epoch 12 | loss: 0.004   | val_0_auc: 0.94909 | val_0_f1_score: 0.63309 |  0:02:48s\n",
      "epoch 13 | loss: 0.00394 | val_0_auc: 0.97    | val_0_f1_score: 0.74194 |  0:03:00s\n",
      "epoch 14 | loss: 0.00403 | val_0_auc: 0.95847 | val_0_f1_score: 0.72441 |  0:03:13s\n",
      "epoch 15 | loss: 0.00371 | val_0_auc: 0.93979 | val_0_f1_score: 0.7377  |  0:03:25s\n",
      "epoch 16 | loss: 0.00357 | val_0_auc: 0.9426  | val_0_f1_score: 0.72    |  0:03:38s\n",
      "epoch 17 | loss: 0.0034  | val_0_auc: 0.94677 | val_0_f1_score: 0.77419 |  0:03:50s\n",
      "epoch 18 | loss: 0.0037  | val_0_auc: 0.9126  | val_0_f1_score: 0.72581 |  0:04:02s\n",
      "epoch 19 | loss: 0.00348 | val_0_auc: 0.92933 | val_0_f1_score: 0.72131 |  0:04:14s\n",
      "epoch 20 | loss: 0.00367 | val_0_auc: 0.95517 | val_0_f1_score: 0.77165 |  0:04:27s\n",
      "epoch 21 | loss: 0.00339 | val_0_auc: 0.93315 | val_0_f1_score: 0.75806 |  0:04:39s\n",
      "epoch 22 | loss: 0.00335 | val_0_auc: 0.93889 | val_0_f1_score: 0.7874  |  0:04:52s\n",
      "epoch 23 | loss: 0.0034  | val_0_auc: 0.93339 | val_0_f1_score: 0.768   |  0:05:05s\n",
      "epoch 24 | loss: 0.00316 | val_0_auc: 0.90966 | val_0_f1_score: 0.73438 |  0:05:17s\n",
      "epoch 25 | loss: 0.00313 | val_0_auc: 0.91689 | val_0_f1_score: 0.63793 |  0:05:29s\n",
      "epoch 26 | loss: 0.00314 | val_0_auc: 0.94677 | val_0_f1_score: 0.7619  |  0:05:41s\n",
      "epoch 27 | loss: 0.0032  | val_0_auc: 0.93501 | val_0_f1_score: 0.68148 |  0:05:53s\n",
      "epoch 28 | loss: 0.00303 | val_0_auc: 0.94765 | val_0_f1_score: 0.73171 |  0:06:05s\n",
      "epoch 29 | loss: 0.00321 | val_0_auc: 0.95701 | val_0_f1_score: 0.76692 |  0:06:16s\n",
      "epoch 30 | loss: 0.00311 | val_0_auc: 0.96432 | val_0_f1_score: 0.76336 |  0:06:28s\n",
      "epoch 31 | loss: 0.00304 | val_0_auc: 0.91826 | val_0_f1_score: 0.59829 |  0:06:40s\n",
      "epoch 32 | loss: 0.00318 | val_0_auc: 0.94438 | val_0_f1_score: 0.624   |  0:06:51s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_f1_score = 0.7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 06:20:21,340] Trial 9 finished with value: 0.7874015748031497 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 4, 'gamma': 2.0, 'lambda_sparse': 1.7746607930209065e-05, 'lr': 0.0008677580582246937}. Best is trial 4 with value: 0.8421052631578947.\n"
     ]
    }
   ],
   "source": [
    "# 4. Run Tuning (the Optuna Study)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")  # Creates a new study\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True)  # Run with progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: lightblue; font-size: 16px; font-weight: bold;'>üìÇSaved Optuna study to: tuning_results/tabnet_study.pkl</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color: lightblue; font-size: 16px; font-weight: bold;'>üìÇSaved best params to: tuning_results/tabnet_best_params.csv</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from utils.style_utils import styled_print \n",
    "\n",
    "# Save study object\n",
    "joblib.dump(study, \"tuning_results/tabnet_study.pkl\")\n",
    "styled_print(\"üìÇSaved Optuna study to: tuning_results/tabnet_study.pkl\")\n",
    "\n",
    "# Save best params\n",
    "best_params = study.best_params\n",
    "pd.DataFrame([best_params]).to_csv(\"tuning_results/tabnet_best_params.csv\", index=False)\n",
    "styled_print(\"üìÇSaved best params to: tuning_results/tabnet_best_params.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Re-train TabNet on Train+Val using Best Params**\n",
    "\n",
    "After finding the optimal configuration, we merged the training and validation sets to retrain the TabNet model from scratch with the best hyperparameters.\n",
    "\n",
    "Key details:\n",
    "\n",
    "- Model: TabNetClassifier with best parameters from Optuna\n",
    "\n",
    "- Dataset: Full Train + Val (70% + 15% of original)\n",
    "\n",
    "- Training setup: Same as before (50 epochs max, early stopping, batch size = 256)\n",
    "\n",
    "**‚ö†Ô∏è Note: Test data was not used during tuning or retraining. It is held out for final evaluation only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: lightblue; font-size: 16px; font-weight: bold;'> Re-training Tabnet Model Using Best Parameters (NO test evaluation yet!)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color: lightblue; font-size: 16px; font-weight: bold;'> Best Params from Optuna: {'n_d': 8, 'n_a': 32, 'n_steps': 3, 'gamma': 2.0, 'lambda_sparse': 0.005221911135507731, 'lr': 0.00032460570676150257}</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.14228 |  0:00:11s\n",
      "epoch 1  | loss: 0.01925 |  0:00:23s\n",
      "epoch 2  | loss: 0.01201 |  0:00:36s\n",
      "epoch 3  | loss: 0.00975 |  0:00:47s\n",
      "epoch 4  | loss: 0.00906 |  0:00:58s\n",
      "epoch 5  | loss: 0.00835 |  0:01:09s\n",
      "epoch 6  | loss: 0.00773 |  0:01:20s\n",
      "epoch 7  | loss: 0.00762 |  0:01:32s\n",
      "epoch 8  | loss: 0.00694 |  0:01:41s\n",
      "epoch 9  | loss: 0.00671 |  0:01:51s\n",
      "epoch 10 | loss: 0.00644 |  0:02:01s\n",
      "epoch 11 | loss: 0.0063  |  0:02:12s\n",
      "epoch 12 | loss: 0.00605 |  0:02:24s\n",
      "epoch 13 | loss: 0.00575 |  0:02:35s\n",
      "epoch 14 | loss: 0.00568 |  0:02:46s\n",
      "epoch 15 | loss: 0.00551 |  0:02:56s\n",
      "epoch 16 | loss: 0.00529 |  0:03:07s\n",
      "epoch 17 | loss: 0.00553 |  0:03:18s\n",
      "epoch 18 | loss: 0.00534 |  0:03:30s\n",
      "epoch 19 | loss: 0.00529 |  0:03:40s\n",
      "epoch 20 | loss: 0.00516 |  0:03:50s\n",
      "epoch 21 | loss: 0.00497 |  0:04:00s\n",
      "epoch 22 | loss: 0.00493 |  0:04:09s\n",
      "epoch 23 | loss: 0.00478 |  0:04:19s\n",
      "epoch 24 | loss: 0.0046  |  0:04:28s\n",
      "epoch 25 | loss: 0.00475 |  0:04:38s\n",
      "epoch 26 | loss: 0.00483 |  0:04:48s\n",
      "epoch 27 | loss: 0.00446 |  0:04:57s\n",
      "epoch 28 | loss: 0.00436 |  0:05:07s\n",
      "epoch 29 | loss: 0.00429 |  0:05:17s\n",
      "epoch 30 | loss: 0.00434 |  0:05:26s\n",
      "epoch 31 | loss: 0.0042  |  0:05:36s\n",
      "epoch 32 | loss: 0.00412 |  0:05:45s\n",
      "epoch 33 | loss: 0.00414 |  0:05:55s\n",
      "epoch 34 | loss: 0.00406 |  0:06:07s\n",
      "epoch 35 | loss: 0.00421 |  0:06:17s\n",
      "epoch 36 | loss: 0.00386 |  0:06:26s\n",
      "epoch 37 | loss: 0.00401 |  0:06:36s\n",
      "epoch 38 | loss: 0.00388 |  0:06:46s\n",
      "epoch 39 | loss: 0.00386 |  0:06:56s\n",
      "epoch 40 | loss: 0.00395 |  0:07:06s\n",
      "epoch 41 | loss: 0.00383 |  0:07:16s\n",
      "epoch 42 | loss: 0.00364 |  0:07:25s\n",
      "epoch 43 | loss: 0.00386 |  0:07:35s\n",
      "epoch 44 | loss: 0.00378 |  0:07:45s\n",
      "epoch 45 | loss: 0.00366 |  0:07:55s\n",
      "epoch 46 | loss: 0.00372 |  0:08:05s\n",
      "epoch 47 | loss: 0.00357 |  0:08:15s\n",
      "epoch 48 | loss: 0.00351 |  0:08:25s\n",
      "epoch 49 | loss: 0.00374 |  0:08:35s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "styled_print(\" Re-training Tabnet Model Using Best Parameters (NO test evaluation yet!)\")\n",
    "# Get best parameters from Optuna\n",
    "best_params = study.best_params\n",
    "styled_print(f\" Best Params from Optuna: {best_params}\")\n",
    "\n",
    "# Merge train + val\n",
    "X_trainval = np.vstack((X_train_scaled, X_val_scaled))\n",
    "y_trainval = np.hstack((y_train, y_val))\n",
    "\n",
    "# Rebuild model with best hyperparameters\n",
    "tabnet_best = TabNetClassifier(\n",
    "    n_d=best_params[\"n_d\"],\n",
    "    n_a=best_params[\"n_a\"],\n",
    "    n_steps=best_params[\"n_steps\"],\n",
    "    gamma=best_params[\"gamma\"],\n",
    "    lambda_sparse=best_params[\"lambda_sparse\"],\n",
    "    optimizer_params=dict(lr=best_params[\"lr\"]),\n",
    "    device_name=\"cpu\",  \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Retrain on full train+val\n",
    "tabnet_best.fit(\n",
    "    X_trainval, y_trainval,\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tuning_results/tabnet_best_model_after_tuning.zip\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: lightblue; font-size: 16px; font-weight: bold;'>üìÇ Saved final TabNet model after tuning to: tuning_results/tabnet_best_model_after_tuning.zip</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the final retrained TabNet model after tuning\n",
    "tabnet_best.save_model(\"tuning_results/tabnet_best_model_after_tuning\")\n",
    "styled_print(\"üìÇ Saved final TabNet model after tuning to: tuning_results/tabnet_best_model_after_tuning.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: lightblue; font-size: 16px; font-weight: bold;'>üìÇ Training history saved to tuning_results/tabnet_best_history_after_tuning.pkl</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Save the training history dictionary if needed later\n",
    "joblib.dump(tabnet_best.history, \"tuning_results/tabnet_best_history_after_tuning.pkl\")\n",
    "styled_print(\"üìÇ Training history saved to tuning_results/tabnet_best_history_after_tuning.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.Unit Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# Load dataset once for all tests\n",
    "data_path = \"../artifacts/tabnet/data_scaled_nosmote_for_tabnet.pkl\"\n",
    "X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test = joblib.load(data_path)\n",
    "\n",
    "# Dummy trial class for objective test\n",
    "class DummyTrial:\n",
    "    def suggest_categorical(self, name, choices):\n",
    "        return choices[0]\n",
    "    def suggest_int(self, name, low, high):\n",
    "        return low\n",
    "    def suggest_float(self, name, low, high, step=None, log=False):\n",
    "        return low\n",
    "\n",
    "class TestTabNetTuning(unittest.TestCase):\n",
    "\n",
    "    def test_objective_function_returns_float(self):\n",
    "        \"\"\"Test the Optuna objective function returns a float F1-score\"\"\"\n",
    "        from tuning.tabnet_tuning import objective  # Adjust if needed\n",
    "        trial = DummyTrial()\n",
    "        result = objective(trial)\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertGreaterEqual(result, 0)\n",
    "        self.assertLessEqual(result, 1)\n",
    "\n",
    "    def test_best_params_keys(self):\n",
    "        \"\"\"Test that the best params dictionary contains required keys\"\"\"\n",
    "        from tuning_results.tabnet_study import study  # Assuming loaded\n",
    "        best_params = study.best_params\n",
    "        required_keys = {\"n_d\", \"n_a\", \"n_steps\", \"gamma\", \"lambda_sparse\", \"lr\"}\n",
    "        self.assertTrue(required_keys.issubset(set(best_params.keys())))\n",
    "\n",
    "    def test_model_trains_on_best_params(self):\n",
    "        \"\"\"Test that TabNetClassifier trains on full train+val without errors\"\"\"\n",
    "        from tuning_results.tabnet_study import study\n",
    "        best_params = study.best_params\n",
    "\n",
    "        model = TabNetClassifier(\n",
    "            n_d=best_params[\"n_d\"],\n",
    "            n_a=best_params[\"n_a\"],\n",
    "            n_steps=best_params[\"n_steps\"],\n",
    "            gamma=best_params[\"gamma\"],\n",
    "            lambda_sparse=best_params[\"lambda_sparse\"],\n",
    "            optimizer_params=dict(lr=best_params[\"lr\"]),\n",
    "            device_name=\"cpu\"\n",
    "        )\n",
    "\n",
    "        # Merge train + val\n",
    "        X_trainval = np.vstack((X_train_scaled, X_val_scaled))\n",
    "        y_trainval = np.hstack((y_train, y_val))\n",
    "\n",
    "        try:\n",
    "            model.fit(\n",
    "                X_trainval, y_trainval,\n",
    "                max_epochs=1,  # Keep fast for unit test\n",
    "                patience=2,\n",
    "                batch_size=256,\n",
    "                virtual_batch_size=128\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Training crashed with error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
